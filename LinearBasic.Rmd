---
title: "Linear Regression"
author: "Billy"
date: "June 7, 2019"
output: html_document
---

##**1. Background**
**Regression* analysis is a technique that typically uses continuous predictor variable(s) (that is, regressor variables) to predict the variation in a continuous response variable.
Regression analysis uses the method of least squares to determine the values of the linear regression coefficients and the corresponding model.
Linear regression is commonly used to quantify the relationship between two or more variables. It is also used to adjust for confounding.

**The method of least squares* is a technique for estimating a parameter that minimizes the sum of the squared differences between the observed and the predicted values derived from the model. These differences are known as residuals or experimental errors. Experimental errors associated with individual observations are assumed to be independent and normally distributed. It should be noted that analysis of variance, regression analysis, and analysis of covariance are all based
on the method of least squares.

**The linear regression equation* is a mathematical function or model that indicates the linear relationship between a set of predictor variables and a response variable.
Linear regression coefficients are numbers associated with each predictor variable in a linear regression equation that tell how the response variable changes with each unit increase in the predictor variable. The model also gives some sense of the degree of linearity present in the data.

##**2. Case study: Money Ball**
Moneyball: The Art of Winning an Unfair Game is a book by Michael Lewis about the Oakland Athletics (Aâ€™s) baseball team and its general manager, the person tasked with building the team, Billy Beane.
* Scouts, a method for finding low cost players that data predicted would help the team win. Today, this strategy has been adapted by most baseball teams.
**Method for this case study**
* Objective: 
  + using the regression analysis which help to develop strategies to build a competitive baseball team with a constrained budget.
  + determine low cost player but high performance
* Scope: scoring runs
* Out of scope: 2 other important aspects:
+ Pitching
+ Fielding
* Action: divided into 2 separate data:
1) to determine which recorded player-specific statistics predict runs.
2) to examine if players were undervalued based on what our first analysis predicts.

###**2.1 Baseball basics**
Baseball goal:
* Score more runs (points) than other team.
* Each team has 9 batters which have an opportuntity to hit a ball with a bat in a predetermined order.
* A better who goes around the bases and arrives home, scores a run.
* 5 ways to make an run:
  + Bases on balls (BB) - the pitcher throw the ball out of a predefined area.
  + Single - Batter hits ball & gets to first base.
  + Double (2B) - Batter hits the ball and gets to second base.
  + Triple (3B) - Batter hits the ball and gets to third base.
  + Home Run (HR) - Batter hits the ball and goes all the way home and scores a run.
```{r, echo = TRUE}
library(tidyverse)
library(corrplot)
library(caret)
library(Lahman)
Teams %>% 
  filter(yearID %in% 1961:2001) %>%
  mutate(HR_per_game = HR / G,
         R_per_game = R/G) %>%
  ggplot(aes(HR_per_game,R_per_game)) +
  geom_point(alpha = 0.5)
```

```{r, echo = TRUE}
df <- Teams %>% filter(yearID %in% 1961:2001) %>%
  mutate(HR_per_game = HR/G,
         R_per_game = R/G,
         BB_per_game = BB/G,
         X1B_per_game = (H-HR-X2B-X3B)/G,
         X2B_per_game = X2B/G,
         X3B_per_game = X3B/G,
         SB_per_game = SB/G) %>%
  select(yearID,teamID,Rank,HR_per_game,R_per_game, BB_per_game, X1B_per_game, X2B_per_game, X3B_per_game, SB_per_game)
```

```{r}
library(lubridate)
df %>% group_by(yearID) %>% 
  mutate(avg_BB = median(BB_per_game),
         avg_HR = median(HR_per_game)) %>% 
  ggplot(aes(x = yearID,y = avg_HR)) +
  geom_line(color = "blue")+
  geom_line(aes(x = yearID, y = avg_BB), color = "orange")
```

```{r}
library(psych)
describe(df)
```

The objective of baseball is getting more runs (points) than the others. What is the most success contribute to Run per game? 
```{r}
df %>% gather("HR_per_game",
              "BB_per_game",
              "X1B_per_game",
              "X2B_per_game",
              "X3B_per_game",
              "SB_per_game",
              key = "Run_type",
              value = "Run") %>%
  mutate(Run_type = reorder(Run_type,
                            Run,
                            FUN = median)) %>%
  ggplot(aes(x = Run_type,
             y = Run,
             fill = Run_type)) +
  geom_boxplot()
```

```{r}
df %>% gather("HR_per_game",
              "BB_per_game",
              "X1B_per_game",
              "X2B_per_game",
              "X3B_per_game",
              "SB_per_game",
              key = "Run_type",
              value = "Run") %>%
  ggplot(aes(x = Run,
             y = R_per_game,
             fill = factor(Run_type))) +
  geom_point(aes(color = Run_type),alpha = 0.5) +
  geom_smooth(method = "gam") +
  theme_classic() +
  theme(legend.position = "none",
        ) +
  facet_wrap(.~Run_type, scales = "free_x")

```


```{r}
fit_lm <- lm(R_per_game~ 
               HR_per_game +
               BB_per_game +
               X1B_per_game +
               X2B_per_game +
               X3B_per_game +
               SB_per_game,
             data = df)
summary(fit_lm)
hist(fit_lm$residuals)
fit_lm$coefficients
```

The Least Squared Estimation is a realization of random variables, which expected value is 0, and standard deviation sigma. To see this, we will run a Monte Carlo simulation with 1000 replication, random sample size N = 50 and compute regression slope coefficient for each one.
```{r}
B <- 1000
N <- 50
lse <- replicate(B, {
  sample_n(df, N, replace = TRUE) %>%
    lm(R_per_game~ 
               HR_per_game +
               BB_per_game +
               X1B_per_game +
               X2B_per_game +
               X3B_per_game +
               SB_per_game,
             data = .) %>%
    .$coef
})
lse <- data.frame(beta_0 = lse[1,], beta_1 = lse[2,]) 
```

```{r}
library(gridExtra)
p1 <- lse %>% ggplot(aes(beta_0)) + geom_histogram(binwidth = 0.2, color = "black") 
p2 <- lse %>% ggplot(aes(beta_1)) + geom_histogram(binwidth = 0.1, color = "black") 
grid.arrange(p1, p2, ncol = 2)
```





##**2. Case study: is height hereditary?**
```{r, eval=FALSE}
library(HistData)
data("GaltonFamilies")
```

```{r, eval=FALSE}
str(GaltonFamilies)
```

```{r}
set.seed(1983)
galton_heights <- GaltonFamilies %>%
  filter(gender == "male") %>%
  group_by(family) %>%
  sample_n(1) %>%
  ungroup() %>%
  select(father, childHeight) %>%
  rename(son = childHeight)
summary(galton_heights)
```

```{r}
galton_heights %>%
  summarize(mean(father), sd(father),
            mean(son), sd(son)) %>% mutate_if(is.numeric, round, 1)
```

```{r, eval=FALSE}
fit <- lm(son ~ father,
          data = galton_heights)
a <- as.character(round(fit$coefficients[2],2))
b <- as.character(round(fit$coefficients[1],2))
r2 <- as.character(round(summary(fit)$r.squared,2))
eq <- as.character(paste("y = ",a,"x + ",b,", R-squared = ",r2))
eq
```

```{r}
galton_heights %>%
  ggplot(aes(father, son)) +
  geom_point(alpha = 0.5,
             color = "orange")+
  geom_smooth(method = "lm", color = "black") +
  labs(title ="father and son regression",
       subtitle = eq) +
  theme_classic()
```

```{r}
rss <- function(beta0, beta1, data){
  resid <- galton_heights$son - (beta0+beta1*galton_heights$father)
  return(sum(resid^2))
}

beta1 = seq(0,1, len=nrow(galton_heights))
results <- data.frame(beta1 = beta1,
                      rss = sapply(beta1, rss, beta0 = 25))
results %>% ggplot(aes(beta1, rss)) + geom_line() + 
  geom_line(aes(beta1, rss), col=2)
which.min(results$rss)
```

```{r}

```


###**2.1 Correlation coefficient**
The sample correlation coefficient r is a statistic that measures the degree of linear relationship between two sets of numbers and is computed as:
r = Sxy / sqrt(Sxx.Sxy)
```{r}
galton_heights %>% summarize(r = cor(father, son)) %>% pull(r)
```

Sample correlation is a random variable
```{r}
R <- sample_n(galton_heights, 25, replace = TRUE) %>%
  summarize(r = cor(father, son))
R
```

Monte Carlo simulation
```{r}
B <- 1000
N <- 25
R <- replicate(B, {
  sample_n(galton_heights, N, replace = TRUE) %>% 
    summarize(r=cor(father, son)) %>% 
    pull(r)
})
qplot(R, geom = "histogram", binwidth = 0.05, color = I("black"))
```

```{r}
round(mean(R),3)
```

```{r}
round(sd(R),3)
```

```{r}
data.frame(R) %>%
  ggplot(aes(sample = R)) +
  stat_qq() +
  geom_abline(intercept = mean(R),
              slope = sqrt((1 - mean(R)^2)/(N-2)))+
  theme_classic()
```

